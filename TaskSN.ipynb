{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import seaborn as sns\n",
    "from tenacity import retry, wait_exponential\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "import evaluate\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained flan t5 small with dialogsum dataset (Task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/Users/inbanerj/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940b6bfed7ca4881b36db526e255af2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "model_name='google/flan-t5-small'\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging mock implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(text):\n",
    "    print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "        trainable_model_params = 0\n",
    "        all_model_params = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            all_model_params += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_model_params += param.numel()\n",
    "        return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization task (Task 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_generator(index):\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        prompt = f\"\"\"\n",
    "        Summarize the following conversation.\n",
    "        {dialogue}\n",
    "        Summary:\n",
    "        \"\"\"\n",
    "        inputs = tokenizer(prompt, return_tensors='pt')\n",
    "        output = tokenizer.decode(\n",
    "        original_model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=200,\n",
    "            )[0], \n",
    "            skip_special_tokens=True\n",
    "            )\n",
    "        dash_line = '-'.join('' for x in range(100))\n",
    "        log(dash_line)\n",
    "        log(f'INPUT PROMPT:\\n{prompt}')\n",
    "        log(dash_line)\n",
    "        log(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "        log(dash_line)\n",
    "        log(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answer task (Task 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer(query):\n",
    "        prompt = f\"\"\"\n",
    "        Answer the query in brief and a catchy manner in about 100 characters\n",
    "        {query}\n",
    "        \"\"\"\n",
    "        inputs = tokenizer(prompt, return_tensors='pt')\n",
    "        output = tokenizer.decode(\n",
    "        original_model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=200,\n",
    "            )[0], \n",
    "            skip_special_tokens=True\n",
    "            )\n",
    "        log(f'Generated Answer:\\n{output}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify if summary generation is working (Task 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "        Summarize the following conversation.\n",
      "        #Person1#: What's the matter, Bill? You look kind of pale.\n",
      "#Person2#: Oh, I'm just tired.\n",
      "#Person1#: Why?\n",
      "#Person2#: Well, I've been working until around ten every night this week.\n",
      "#Person1#: You should go home at quitting time today and take it easy.\n",
      "#Person2#: Yes. I think I will.\n",
      "#Person1#: That's good. Say, how's your brother?\n",
      "#Person2#: He's fine, but he is awfully busy. He went to the States on a business trip two weeks ago.\n",
      "#Person1#: Oh, really? Is he back yet?\n",
      "#Person2#: No, he won't come back for several more weeks.\n",
      "#Person1#: Wow! He must have a lot to do there.\n",
      "#Person2#: Yes, he does.\n",
      "#Person1#: I want to be sure of the time because I'm going to meet a friend at five o'clock sharp.\n",
      "#Person2#: Well, my watch says 4:30, and that time should be right. I set it with the radio yesterday.\n",
      "#Person1#: Good.\n",
      "        Summary:\n",
      "        \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Bill is tired. Bill and #Person1# talk about Bill's brother.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Bill, I'm sorry.\n"
     ]
    }
   ],
   "source": [
    "index = 105\n",
    "summary_generator(index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify if question answer is working (Task 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      "scott scott\n"
     ]
    }
   ],
   "source": [
    "query = \" Who is the best footballer of all times\"\n",
    "question_answer(query)\n",
    "\n",
    "# The question answer model is giving responses but not good or accurate ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      "You can use a syringe to procrastinate.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is a good way to procrastinate\"\n",
    "question_answer(query)\n",
    "\n",
    "# Again not very relevant responses but this is expected for a google flan t5 small model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programatically print the parameters (Task 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 76961152\n",
      "all model parameters: 76961152\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "log(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programatically print the names of all the model layers and their dimensions (Task 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 512)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "log(original_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the tensor in final layer to all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.bfloat16,\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "original_model.decoder.final_layer_norm.weight=torch.nn.Parameter(torch.zeros(512, dtype=torch.bfloat16))\n",
    "print(original_model.decoder.final_layer_norm.weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify if the QnA model is still working (Task 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is a good way to procrastinate\"\n",
    "question_answer(query)\n",
    "\n",
    "# Model is alive but unable wo generate any response with weights zero"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing the model decoder final layer with smaller dimensions (256) Task 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.bfloat16, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "original_model.decoder.final_layer_norm.weight=torch.nn.Parameter(torch.zeros(256, dtype=torch.bfloat16))\n",
    "print(original_model.decoder.final_layer_norm.weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the SQUAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/wget/manifests/1.21.4\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching dependencies for wget: \u001b[32mlibidn2\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libidn2/manifests/2.3.4_1-1\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/03ad193177f4e7d05ee2ed19a455028cb5fbf7ea1a812d88f18f5e9e8b4a4d43--libidn2-2.3.4_1-1.bottle_manifest.json\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mlibidn2\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libidn2/blobs/sha256:b044c66cc0\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/9977cd3f036c73f93cc7e17345c9c5faba6d7b0057b39239f374efd66f656621--libidn2--2.3.4_1.arm64_ventura.bottle.1.tar.gz\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mwget\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/wget/blobs/sha256:c7b3fe54045aa\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling dependencies for wget: \u001b[32mlibidn2\u001b[39m\u001b[0m\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling wget dependency: \u001b[32mlibidn2\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libidn2/manifests/2.3.4_1-1\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/03ad193177f4e7d05ee2ed19a455028cb5fbf7ea1a812d88f18f5e9e8b4a4d43--libidn2-2.3.4_1-1.bottle_manifest.json\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring libidn2--2.3.4_1.arm64_ventura.bottle.1.tar.gz\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/libidn2/2.3.4_1: 79 files, 1MB\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling \u001b[32mwget\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring wget--1.21.4.arm64_ventura.bottle.tar.gz\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/wget/1.21.4: 91 files, 4.4MB\n",
      "\u001b[34m==>\u001b[0m \u001b[1mRunning `brew cleanup wget`...\u001b[0m\n",
      "Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\n",
      "Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mUpgrading 3 dependents of upgraded formulae:\u001b[0m\n",
      "Disable this behaviour by setting HOMEBREW_NO_INSTALLED_DEPENDENTS_CHECK.\n",
      "Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n",
      "gnutls 3.8.1 -> 3.8.2, qemu 8.1.0_3 -> 8.1.2, lima 0.17.2 -> 0.18.0\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/gnutls/manifests/3.8.2\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching dependencies for gnutls: \u001b[32mgmp\u001b[39m, \u001b[32mgettext\u001b[39m, \u001b[32mp11-kit\u001b[39m, \u001b[32mopenssl@3\u001b[39m, \u001b[32mlibevent\u001b[39m, \u001b[32mlibnghttp2\u001b[39m and \u001b[32munbound\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/gmp/manifests/6.3.0\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mgmp\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/gmp/blobs/sha256:98c163edfbe7bd\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/gettext/manifests/0.22.3\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mgettext\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/gettext/blobs/sha256:9bf5b0ea5f\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/p11-kit/manifests/0.25.3\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mp11-kit\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/p11-kit/blobs/sha256:f965f464d9\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/openssl/3/manifests/3.1.4\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mopenssl@3\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/openssl/3/blobs/sha256:1ff0549b\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libevent/manifests/2.1.12_1\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/68b113f9ab63db45f4e1860de522ce2ca4fa081eb3c0d5c7d6005a35c3cf8d06--libevent-2.1.12_1.bottle_manifest.json\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mlibevent\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libevent/blobs/sha256:a75d453a7\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libnghttp2/manifests/1.58.0\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mlibnghttp2\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libnghttp2/blobs/sha256:73d3d70\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/unbound/manifests/1.19.0\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32munbound\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/unbound/blobs/sha256:5c00016807\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mgnutls\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/gnutls/blobs/sha256:48cbe35994d\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/qemu/manifests/8.1.2\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching dependencies for qemu: \u001b[32mlibssh\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libssh/manifests/0.10.5_1\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/85e02030f335c38124a70d78695341dc20a2d4f47896f4d44c260da135922c44--libssh-0.10.5_1.bottle_manifest.json\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mlibssh\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libssh/blobs/sha256:5b5925ae5e0\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/f7fb8eec7f8aec779c0b7f18de4d86bd848e6abd17364c6c1cacc1ad42602155--libssh--0.10.5_1.arm64_ventura.bottle.tar.gz\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mqemu\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/qemu/blobs/sha256:a56c3c3132876\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/lima/manifests/0.18.0\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mlima\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/lima/blobs/sha256:f6fab71646bd6\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m==>\u001b[0m \u001b[1mUpgrading \u001b[32mgnutls\u001b[39m\n",
      "  3.8.1 -> 3.8.2 \n",
      "\u001b[0m\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling dependencies for gnutls: \u001b[32mgmp\u001b[39m, \u001b[32mgettext\u001b[39m, \u001b[32mp11-kit\u001b[39m, \u001b[32mopenssl@3\u001b[39m, \u001b[32mlibevent\u001b[39m, \u001b[32mlibnghttp2\u001b[39m and \u001b[32munbound\u001b[39m\u001b[0m\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling gnutls dependency: \u001b[32mgmp\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/gmp/manifests/6.3.0\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/70a72a71216843d66a953c06ff6337445ce9bc94fae9f0e301e2f59005274a8e--gmp-6.3.0.bottle_manifest.json\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring gmp--6.3.0.arm64_ventura.bottle.tar.gz\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/gmp/6.3.0: 21 files, 3.3MB\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling gnutls dependency: \u001b[32mgettext\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/gettext/manifests/0.22.3\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/c39063a94fdbad486d1b2e0bb3832dc7d7d7533dd55e07a10322af82dc115e44--gettext-0.22.3.bottle_manifest.json\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring gettext--0.22.3.arm64_ventura.bottle.tar.gz\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/gettext/0.22.3: 2,040 files, 21.9MB\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling gnutls dependency: \u001b[32mp11-kit\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/p11-kit/manifests/0.25.3\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/cc547bf2f72da03680090015f5b720aff280ce33de9c33783a69c24fe97a4246--p11-kit-0.25.3.bottle_manifest.json\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring p11-kit--0.25.3.arm64_ventura.bottle.tar.gz\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/p11-kit/0.25.3: 28 files, 4.2MB\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling gnutls dependency: \u001b[32mopenssl@3\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/openssl/3/manifests/3.1.4\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/528fccab152aa17857e73f54f268d413bf94e22188c0e4ff17a6c497d2783b88--openssl@3-3.1.4.bottle_manifest.json\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring openssl@3--3.1.4.arm64_ventura.bottle.tar.gz\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/openssl@3/3.1.4: 6,496 files, 28.4MB\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling gnutls dependency: \u001b[32mlibevent\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libevent/manifests/2.1.12_1\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/68b113f9ab63db45f4e1860de522ce2ca4fa081eb3c0d5c7d6005a35c3cf8d06--libevent-2.1.12_1.bottle_manifest.json\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring libevent--2.1.12_1.arm64_ventura.bottle.tar.gz\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/libevent/2.1.12_1: 57 files, 2.2MB\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling gnutls dependency: \u001b[32mlibnghttp2\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libnghttp2/manifests/1.58.0\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/9b2081d73959d4308c84740344d5f8169bbb9125dd76d20a71556a11a0171253--libnghttp2-1.58.0.bottle_manifest.json\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring libnghttp2--1.58.0.arm64_ventura.bottle.tar.gz\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/libnghttp2/1.58.0: 13 files, 739.2KB\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling gnutls dependency: \u001b[32munbound\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/unbound/manifests/1.19.0\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/5ef4c435591de61234a87b911b8f2793a67aea99fa163a6c156bb9303f7fac11--unbound-1.19.0.bottle_manifest.json\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling dependencies for unbound: \u001b[32mlibevent\u001b[39m\u001b[0m\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling unbound dependency: \u001b[32mlibevent\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libevent/manifests/2.1.12_1\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/68b113f9ab63db45f4e1860de522ce2ca4fa081eb3c0d5c7d6005a35c3cf8d06--libevent-2.1.12_1.bottle_manifest.json\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring libevent--2.1.12_1.arm64_ventura.bottle.tar.gz\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/libevent/2.1.12_1: 57 files, 2.2MB\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling \u001b[32munbound\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring unbound--1.19.0.arm64_ventura.bottle.tar.gz\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/unbound/1.19.0: 58 files, 5.9MB\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling \u001b[32mgnutls\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring gnutls--3.8.2.arm64_ventura.bottle.tar.gz\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/gnutls/3.8.2: 1,290 files, 10.8MB\n",
      "\u001b[34m==>\u001b[0m \u001b[1mRunning `brew cleanup gnutls`...\u001b[0m\n",
      "Removing: /opt/homebrew/Cellar/gnutls/3.8.1... (1,284 files, 10.7MB)\n",
      "Removing: /Users/inbanerj/Library/Caches/Homebrew/gnutls--3.8.1... (3.0MB)\n",
      "\u001b[32m==>\u001b[0m \u001b[1mUpgrading \u001b[32mqemu\u001b[39m\n",
      "  8.1.0_3 -> 8.1.2 \n",
      "\u001b[0m\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling dependencies for qemu: \u001b[32mlibidn2\u001b[39m, \u001b[32mlibevent\u001b[39m and \u001b[32mlibssh\u001b[39m\u001b[0m\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling qemu dependency: \u001b[32mlibidn2\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libidn2/manifests/2.3.4_1-1\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/03ad193177f4e7d05ee2ed19a455028cb5fbf7ea1a812d88f18f5e9e8b4a4d43--libidn2-2.3.4_1-1.bottle_manifest.json\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring libidn2--2.3.4_1.arm64_ventura.bottle.1.tar.gz\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/libidn2/2.3.4_1: 79 files, 1MB\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling qemu dependency: \u001b[32mlibevent\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libevent/manifests/2.1.12_1\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/68b113f9ab63db45f4e1860de522ce2ca4fa081eb3c0d5c7d6005a35c3cf8d06--libevent-2.1.12_1.bottle_manifest.json\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring libevent--2.1.12_1.arm64_ventura.bottle.tar.gz\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/libevent/2.1.12_1: 57 files, 2.2MB\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling qemu dependency: \u001b[32mlibssh\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libssh/manifests/0.10.5_1\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/85e02030f335c38124a70d78695341dc20a2d4f47896f4d44c260da135922c44--libssh-0.10.5_1.bottle_manifest.json\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring libssh--0.10.5_1.arm64_ventura.bottle.tar.gz\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/libssh/0.10.5_1: 23 files, 1.3MB\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling \u001b[32mqemu\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring qemu--8.1.2.arm64_ventura.bottle.tar.gz\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/qemu/8.1.2: 162 files, 534.1MB\n",
      "\u001b[34m==>\u001b[0m \u001b[1mRunning `brew cleanup qemu`...\u001b[0m\n",
      "Removing: /opt/homebrew/Cellar/qemu/8.1.0_3... (162 files, 533.9MB)\n",
      "Removing: /Users/inbanerj/Library/Caches/Homebrew/qemu--8.1.0_3... (100.4MB)\n",
      "\u001b[32m==>\u001b[0m \u001b[1mUpgrading \u001b[32mlima\u001b[39m\n",
      "  0.17.2 -> 0.18.0 \n",
      "\u001b[0m\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling dependencies for lima: \u001b[32mlibidn2\u001b[39m, \u001b[32mlibevent\u001b[39m and \u001b[32mlibssh\u001b[39m\u001b[0m\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling lima dependency: \u001b[32mlibidn2\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libidn2/manifests/2.3.4_1-1\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/03ad193177f4e7d05ee2ed19a455028cb5fbf7ea1a812d88f18f5e9e8b4a4d43--libidn2-2.3.4_1-1.bottle_manifest.json\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring libidn2--2.3.4_1.arm64_ventura.bottle.1.tar.gz\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/libidn2/2.3.4_1: 79 files, 1MB\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling lima dependency: \u001b[32mlibevent\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libevent/manifests/2.1.12_1\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/68b113f9ab63db45f4e1860de522ce2ca4fa081eb3c0d5c7d6005a35c3cf8d06--libevent-2.1.12_1.bottle_manifest.json\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring libevent--2.1.12_1.arm64_ventura.bottle.tar.gz\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/libevent/2.1.12_1: 57 files, 2.2MB\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling lima dependency: \u001b[32mlibssh\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libssh/manifests/0.10.5_1\u001b[0m\n",
      "Already downloaded: /Users/inbanerj/Library/Caches/Homebrew/downloads/85e02030f335c38124a70d78695341dc20a2d4f47896f4d44c260da135922c44--libssh-0.10.5_1.bottle_manifest.json\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring libssh--0.10.5_1.arm64_ventura.bottle.tar.gz\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/libssh/0.10.5_1: 23 files, 1.3MB\n",
      "\u001b[32m==>\u001b[0m \u001b[1mInstalling \u001b[32mlima\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring lima--0.18.0.arm64_ventura.bottle.tar.gz\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mCaveats\u001b[0m\n",
      "zsh completions have been installed to:\n",
      "  /opt/homebrew/share/zsh/site-functions\n",
      "\u001b[34m==>\u001b[0m \u001b[1mSummary\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/lima/0.18.0: 104 files, 169.9MB\n",
      "\u001b[34m==>\u001b[0m \u001b[1mRunning `brew cleanup lima`...\u001b[0m\n",
      "Removing: /opt/homebrew/Cellar/lima/0.17.2... (111 files, 167.4MB)\n",
      "Removing: /Users/inbanerj/Library/Caches/Homebrew/lima--0.17.2... (49.4MB)\n",
      "\u001b[32m==>\u001b[0m \u001b[1mChecking for dependents of upgraded formulae...\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNo broken dependents found!\u001b[0m\n",
      "\u001b[32m==>\u001b[0m \u001b[1m`brew cleanup` has not been run in the last 30 days, running now...\u001b[0m\n",
      "Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\n",
      "Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n",
      "Removing: /Users/inbanerj/Library/Caches/Homebrew/cffi--1.15.1... (165.3KB)\n",
      "Removing: /opt/homebrew/Cellar/gettext/0.21.1... (1,983 files, 20.9MB)\n",
      "Removing: /Users/inbanerj/Library/Caches/Homebrew/gettext--0.21.1... (8.8MB)\n",
      "Removing: /Users/inbanerj/Library/Caches/Homebrew/glib--2.78.0... (6.6MB)\n",
      "Removing: /opt/homebrew/Cellar/gmp/6.2.1_1... (21 files, 3.2MB)\n",
      "Removing: /opt/homebrew/Cellar/libevent/2.1.12... (57 files, 2.1MB)\n",
      "Removing: /opt/homebrew/Cellar/libnghttp2/1.56.0... (13 files, 734KB)\n",
      "Removing: /Users/inbanerj/Library/Caches/Homebrew/libnghttp2--1.56.0... (215.7KB)\n",
      "Removing: /Users/inbanerj/Library/Caches/Homebrew/oci-cli--3.33.0... (23.4MB)\n",
      "Removing: /opt/homebrew/Cellar/openssl@3/3.1.2... (6,495 files, 28.4MB)\n",
      "Removing: /Users/inbanerj/Library/Caches/Homebrew/openssl@3--3.1.2... (7.7MB)\n",
      "Removing: /opt/homebrew/Cellar/p11-kit/0.25.0... (67 files, 4.8MB)\n",
      "Removing: /Users/inbanerj/Library/Caches/Homebrew/p11-kit--0.25.0... (1MB)\n",
      "Removing: /Users/inbanerj/Library/Caches/Homebrew/python-cryptography--41.0.3... (1MB)\n",
      "Removing: /Users/inbanerj/Library/Caches/Homebrew/python@3.11--3.11.5... (14.9MB)\n",
      "Removing: /Users/inbanerj/Library/Caches/Homebrew/sqlite--3.43.1... (2.1MB)\n",
      "Removing: /opt/homebrew/Cellar/unbound/1.18.0_1... (58 files, 5.9MB)\n",
      "Removing: /Users/inbanerj/Library/Caches/Homebrew/unbound--1.18.0_1... (2.9MB)\n",
      "Removing: /Users/inbanerj/Library/Caches/Homebrew/xz--5.4.4... (660.5KB)\n",
      "Removing: /Users/inbanerj/Library/Logs/Homebrew/glib... (64B)\n",
      "Removing: /Users/inbanerj/Library/Logs/Homebrew/openssl@3... (64B)\n",
      "Removing: /Users/inbanerj/Library/Logs/Homebrew/ca-certificates... (64B)\n",
      "Removing: /Users/inbanerj/Library/Logs/Homebrew/unbound... (64B)\n",
      "Removing: /Users/inbanerj/Library/Logs/Homebrew/python@3.11... (2 files, 2.5KB)\n",
      "Removing: /Users/inbanerj/Library/Logs/Homebrew/python@3.8... (3 files, 243.7KB)\n",
      "Removing: /Users/inbanerj/Library/Logs/Homebrew/guile... (64B)\n",
      "Removing: /Users/inbanerj/Library/Logs/Homebrew/gnutls... (64B)\n",
      "Removing: /opt/homebrew/lib/python3.9/site-packages/__pycache__/pylab.cpython-39.pyc... (222B)\n",
      "Removing: /opt/homebrew/lib/python3.9/site-packages/__pycache__/typing_extensions.cpython-39.pyc... (67.3KB)\n",
      "Pruned 0 symbolic links and 2 directories from /opt/homebrew\n",
      "\u001b[32m==>\u001b[0m \u001b[1mCaveats\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mlima\u001b[0m\n",
      "zsh completions have been installed to:\n",
      "  /opt/homebrew/share/zsh/site-functions\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2023-11-19 08:47:08--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
      "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.110.153, 185.199.111.153, 185.199.109.153, ...\n",
      "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.110.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 42123633 (40M) [application/json]\n",
      "Saving to: ‘train.json’\n",
      "\n",
      "train.json          100%[===================>]  40.17M  24.1MB/s    in 1.7s    \n",
      "\n",
      "2023-11-19 08:47:13 (24.1 MB/s) - ‘train.json’ saved [42123633/42123633]\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2023-11-19 08:47:13--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
      "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.111.153, 185.199.109.153, 185.199.108.153, ...\n",
      "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.111.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4370528 (4.2M) [application/json]\n",
      "Saving to: ‘dev.json’\n",
      "\n",
      "dev.json            100%[===================>]   4.17M  12.9MB/s    in 0.3s    \n",
      "\n",
      "2023-11-19 08:47:14 (12.9 MB/s) - ‘dev.json’ saved [4370528/4370528]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the data for rag implementation\n",
    "!brew install wget\n",
    "!mkdir -p local_cache\n",
    "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O train.json\n",
    "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O dev.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read JSON files of the SQUAD dataset on to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_dataframe_with_titles(json_data):\n",
    "    qas = []\n",
    "    context = []\n",
    "    is_impossible = []\n",
    "    answers = []\n",
    "    titles = []\n",
    "\n",
    "    for article in json_data['data']:\n",
    "        title = article['title']\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                qas.append(qa['question'].strip())\n",
    "                context.append(paragraph['context'])\n",
    "                is_impossible.append(qa['is_impossible'])\n",
    "                \n",
    "                ans_list = []\n",
    "                for ans in qa['answers']:\n",
    "                    ans_list.append(ans['text'])\n",
    "                answers.append(ans_list)\n",
    "                titles.append(title)\n",
    "\n",
    "    df = pd.DataFrame({'title': titles, 'question': qas, 'context': context, 'is_impossible': is_impossible, 'answers': answers})\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a diverse sample of the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diverse_sample(df, sample_size=100, random_state=42):\n",
    "\n",
    "    sample_df = df.groupby(['title', 'is_impossible']).apply(lambda x: x.sample(min(len(x), max(1, sample_size // 50)), random_state=random_state)).reset_index(drop=True)\n",
    "    \n",
    "    if len(sample_df) < sample_size:\n",
    "        remaining_sample_size = sample_size - len(sample_df)\n",
    "        remaining_df = df.drop(sample_df.index).sample(remaining_sample_size, random_state=random_state)\n",
    "        sample_df = pd.concat([sample_df, remaining_df]).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    return sample_df.sample(min(sample_size, len(sample_df)), random_state=random_state).reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the train and validation DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = json_to_dataframe_with_titles(json.load(open('train.json')))\n",
    "val_df = json_to_dataframe_with_titles(json.load(open('dev.json')))\n",
    "\n",
    "df = get_diverse_sample(val_df, sample_size=100, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get prompt messages with additional context addition (SQUAD dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(row):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Answer the following Question based on the Context only. Only answer from the Context. If you don't know the answer, say 'I don't know'.\n",
    "    Question: {row.question}\\n\\n\n",
    "    Context: {row.context}\\n\\n\n",
    "    Answer:\\n\"\"\",\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload the Google flan t5 small model (Task 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/Users/inbanerj/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776cc8825d7448d5a649bbf43f4e1536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name='google/flan-t5-small'\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>is_impossible</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scottish_Parliament</td>\n",
       "      <td>What consequence of establishing the Scottish ...</td>\n",
       "      <td>A procedural consequence of the establishment ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[able to vote on domestic legislation that app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Imperialism</td>\n",
       "      <td>Imperialism is less often associated with whic...</td>\n",
       "      <td>The principles of imperialism are often genera...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Economic_inequality</td>\n",
       "      <td>What issues can't prevent women from working o...</td>\n",
       "      <td>When a person’s capabilities are lowered, they...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Southern_California</td>\n",
       "      <td>What county are Los Angeles, Orange, San Diego...</td>\n",
       "      <td>Its counties of Los Angeles, Orange, San Diego...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>French_and_Indian_War</td>\n",
       "      <td>When was the deportation of Canadians?</td>\n",
       "      <td>Britain gained control of French Canada and Ac...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Geology</td>\n",
       "      <td>In the layered Earth model, what is the inner ...</td>\n",
       "      <td>Seismologists can use the arrival times of sei...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Prime_number</td>\n",
       "      <td>What type of value would the Basel function ha...</td>\n",
       "      <td>The zeta function is closely related to prime ...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Fresno,_California</td>\n",
       "      <td>What does the San Joaquin Valley Railroad cros...</td>\n",
       "      <td>Passenger rail service is provided by Amtrak S...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Victoria_(Australia)</td>\n",
       "      <td>What party rules in Melbourne's inner regions?</td>\n",
       "      <td>The centre-left Australian Labor Party (ALP), ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[The Greens, Australian Greens, Greens]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Immune_system</td>\n",
       "      <td>The speed of the killing response of the human...</td>\n",
       "      <td>In humans, this response is activated by compl...</td>\n",
       "      <td>False</td>\n",
       "      <td>[signal amplification, signal amplification, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    title                                           question  \\\n",
       "0     Scottish_Parliament  What consequence of establishing the Scottish ...   \n",
       "1             Imperialism  Imperialism is less often associated with whic...   \n",
       "2     Economic_inequality  What issues can't prevent women from working o...   \n",
       "3     Southern_California  What county are Los Angeles, Orange, San Diego...   \n",
       "4   French_and_Indian_War             When was the deportation of Canadians?   \n",
       "..                    ...                                                ...   \n",
       "95                Geology  In the layered Earth model, what is the inner ...   \n",
       "96           Prime_number  What type of value would the Basel function ha...   \n",
       "97     Fresno,_California  What does the San Joaquin Valley Railroad cros...   \n",
       "98   Victoria_(Australia)     What party rules in Melbourne's inner regions?   \n",
       "99          Immune_system  The speed of the killing response of the human...   \n",
       "\n",
       "                                              context  is_impossible  \\\n",
       "0   A procedural consequence of the establishment ...          False   \n",
       "1   The principles of imperialism are often genera...           True   \n",
       "2   When a person’s capabilities are lowered, they...           True   \n",
       "3   Its counties of Los Angeles, Orange, San Diego...           True   \n",
       "4   Britain gained control of French Canada and Ac...           True   \n",
       "..                                                ...            ...   \n",
       "95  Seismologists can use the arrival times of sei...           True   \n",
       "96  The zeta function is closely related to prime ...           True   \n",
       "97  Passenger rail service is provided by Amtrak S...           True   \n",
       "98  The centre-left Australian Labor Party (ALP), ...          False   \n",
       "99  In humans, this response is activated by compl...          False   \n",
       "\n",
       "                                              answers  \n",
       "0   [able to vote on domestic legislation that app...  \n",
       "1                                                  []  \n",
       "2                                                  []  \n",
       "3                                                  []  \n",
       "4                                                  []  \n",
       "..                                                ...  \n",
       "95                                                 []  \n",
       "96                                                 []  \n",
       "97                                                 []  \n",
       "98            [The Greens, Australian Greens, Greens]  \n",
       "99  [signal amplification, signal amplification, s...  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_json(\"100_val.json\", orient=\"records\", lines=True)\n",
    "df = pd.read_json(\"100_val.json\", orient=\"records\", lines=True)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data for training (fine-tuning). Add answer to the context. Task 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Answer the following Question based on the Context only. Only answer from the Context. If you don't know the answer, say 'I don't know'.\\n            Question: How much was the Labour majority reduced by?\\n\\n\\n            Context: Labour improved its performance in 1987, gaining 20 seats and so reducing the Conservative majority from 143 to 102. They were now firmly re-established as the second political party in Britain as the Alliance had once again failed to make a breakthrough with seats. A merger of the SDP and Liberals formed the Liberal Democrats. Following the 1987 election, the National Executive Committee resumed disciplinary action against members of Militant, who remained in the party, leading to further expulsions of their activists and the two MPs who supported the group.\\n\\n\\n            Answer:\\n\"}, {\"role\": \"assistant\", \"content\": \"I don't know\"}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A procedural consequence of the establishment of the Scottish Parliament is that Scottish MPs sitting in the UK House of Commons are able to vote on domestic legislation that applies only to England, Wales and Northern Ireland – whilst English, Scottish, Welsh and Northern Irish Westminster MPs are unable to vote on the domestic legislation of the Scottish Parliament. This phenomenon is known as the West Lothian question and has led to criticism. Following the Conservative victory in the 2015 UK election, standing orders of the House of Commons were changed to give MPs representing English constituencies a new \"veto\" over laws only affecting England.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataframe_to_jsonl(df):\n",
    "    def create_jsonl_entry(row):\n",
    "        answer = row[\"answers\"][0] if row[\"answers\"] else \"I don't know\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Answer the following Question based on the Context only. Only answer from the Context. If you don't know the answer, say 'I don't know'.\n",
    "            Question: {row.question}\\n\\n\n",
    "            Context: {row.context}\\n\\n\n",
    "            Answer:\\n\"\"\",\n",
    "            },\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "        ]\n",
    "        return json.dumps({\"messages\": messages})\n",
    "\n",
    "    jsonl_output = df.apply(create_jsonl_entry, axis=1)\n",
    "    print(jsonl_output[1])\n",
    "    return \"\\n\".join(jsonl_output)\n",
    "\n",
    "train_sample = get_diverse_sample(train_df, sample_size=100, random_state=42)\n",
    "\n",
    "with open(\"100_train.jsonl\", \"w\") as f:\n",
    "    f.write(dataframe_to_jsonl(train_sample))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context addition Task 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer2(query, context):\n",
    "        prompt = f\"\"\"\n",
    "        Answer the query in brief and a catchy manner in about 100 characters\n",
    "        {query}\n",
    "        Use the additional information given in the context below to come up an answer\n",
    "        {context}\n",
    "        \"\"\"\n",
    "        inputs = tokenizer(prompt, return_tensors='pt')\n",
    "        output = tokenizer.decode(\n",
    "        original_model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=200,\n",
    "            )[0], \n",
    "            skip_special_tokens=True\n",
    "            )\n",
    "        log(f'Generation with Context added:\\n{output}')\n",
    "        return str(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample outputs to show the model is coming up with responses with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation with Context added:\n",
      "unable to vote on the domestic legislation of the Scottish Parliament\n",
      "Generation with Context added:\n",
      "Australia\n",
      "Generation with Context added:\n",
      "gender roles and customs\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    df[\"answers\"][i] = question_answer2(df[\"question\"][i], df[\"context\"][i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing responses are working or not by injecting a custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation with Context added:\n",
      "he is widely regarded as one of the most successful and influential actors in the history of Indian cinema\n"
     ]
    }
   ],
   "source": [
    "df[\"question\"][1] = \"Who is Amitabh Bachchan?\"\n",
    "df[\"context\"][1] = \"Amitabh Bachchan, born in 1942, is an Indian film producer, television host, occasional playback singer and former politician, and actor who works in Hindi cinema. In a film career spanning over five decades, he has starred in more than 200 films. Bachchan is widely regarded as one of the most successful and influential actors in the history of Indian cinema\"\n",
    "df[\"answers\"][1] = question_answer2(df[\"question\"][1], df[\"context\"][1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Task 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation framework with ROUGE and BERT_Score\n",
    "# The comparision can be extended to GPT-4 outputs as reference \n",
    "# Additionally we can bertscore for Cosine Similarities.\n",
    "\n",
    "def Evaluatemet(predictions, references):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    results_rouge = rouge.compute(predictions=predictions,references=references)\n",
    "\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    results_bleuscore = bleu.compute(predictions=predictions,references=references)\n",
    "\n",
    "    dash_line = '-'.join('' for x in range(100))\n",
    "    log(dash_line)\n",
    "    log(results_rouge)\n",
    "    log(dash_line)\n",
    "    log(results_bleuscore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation with Context added:\n",
      "unable to vote on the domestic legislation of the Scottish Parliament\n",
      "---------------------------------------------------------------------------------------------------\n",
      "{'rouge1': 0.028985507246376812, 'rouge2': 0.0, 'rougeL': 0.028985507246376812, 'rougeLsum': 0.028985507246376812}\n",
      "---------------------------------------------------------------------------------------------------\n",
      "{'bleu': 0.0, 'precisions': [0.03389830508474576, 0.0, 0.0, 0.0], 'brevity_penalty': 0.9831936762627184, 'length_ratio': 0.9833333333333333, 'translation_length': 59, 'reference_length': 60}\n",
      "Generation with Context added:\n",
      "he is widely regarded as one of the most successful and influential actors in the history of Indian cinema\n",
      "---------------------------------------------------------------------------------------------------\n",
      "{'rouge1': 0.018867924528301886, 'rouge2': 0.0, 'rougeL': 0.018867924528301886, 'rougeLsum': 0.018867924528301886}\n",
      "---------------------------------------------------------------------------------------------------\n",
      "{'bleu': 0.0, 'precisions': [0.022727272727272728, 0.0, 0.0, 0.0], 'brevity_penalty': 0.9664836384946974, 'length_ratio': 0.967032967032967, 'translation_length': 88, 'reference_length': 91}\n",
      "Generation with Context added:\n",
      "gender roles and customs\n",
      "---------------------------------------------------------------------------------------------------\n",
      "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n",
      "---------------------------------------------------------------------------------------------------\n",
      "{'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 21, 'reference_length': 21}\n"
     ]
    }
   ],
   "source": [
    "# While the idea to take the length of prediction in the reference text is not actually correct, this is just for demonstration purposes how the evaluation module will work\n",
    "# More logical rules that keep the prediction and reference lengths the same for BLEU and ROUGE matrices will need to be implemented.\n",
    "# ECSM implementation (BERTScore) can also be done\n",
    "\n",
    "for i in range(3):\n",
    "    df[\"answers\"][i] = question_answer2(df[\"question\"][i], df[\"context\"][i])\n",
    "    predictions = df[\"answers\"][i]\n",
    "    references = df[\"context\"][i][0:len(predictions)]\n",
    "    Evaluatemet(predictions, references)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
